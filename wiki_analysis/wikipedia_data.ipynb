{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2837c99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "import gzip\n",
    "import internetarchive\n",
    "import time\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e99abb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = internetarchive.search_items('collection:WikiHist_html')\n",
    "itemids = [r['identifier'] for r in search][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "690f65c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR_TMP = 'wiki_data_tmp_download'\n",
    "DATA_DIR = 'wiki_data'\n",
    "URL_BASE = 'https://archive.org/download/'\n",
    "FAILURES_FILE = 'failures.txt'\n",
    "\n",
    "class Capturing(list):\n",
    "    def __enter__(self):\n",
    "        self._stdout = sys.stdout\n",
    "        sys.stdout = self._stringio = StringIO()\n",
    "        return self\n",
    "    def __exit__(self, *args):\n",
    "        self.extend(self._stringio.getvalue().splitlines())\n",
    "        del self._stringio    # free up some memory\n",
    "        sys.stdout = self._stdout\n",
    "\n",
    "COLS = ['parentid',\n",
    "        'id',\n",
    "        'cont_username',\n",
    "        'cont_id',\n",
    "        'timestamp', \n",
    "        'format',\n",
    "        'page_id',\n",
    "        'title']\n",
    "\n",
    "def get_csv(itemid):\n",
    "    return f\"{os.path.join(DATA_DIR, itemid)}.csv\"\n",
    "\n",
    "def get_data(itemid):\n",
    "    print(f'======= Downloading data for {itemid} =======')\n",
    "    t_init = time.time()\n",
    "    \n",
    "    if not os.path.isdir(DATA_DIR_TMP):\n",
    "        os.mkdir(DATA_DIR_TMP)\n",
    "\n",
    "    if not os.path.isdir(DATA_DIR):\n",
    "        os.mkdir(DATA_DIR)\n",
    "    \n",
    "    if os.path.isfile(os.path.join(DATA_DIR, f'{itemid}.csv')):\n",
    "        print('Already downloaded.')\n",
    "        return\n",
    "    \n",
    "    t = time.time()\n",
    "    \n",
    "    download_data(itemid)    \n",
    "    \n",
    "    t_new = time.time()\n",
    "    print(f\"======= Downloaded Data in {round(t_new - t, 2)} seconds ========\")\n",
    "    \n",
    "    def save_and_delete_data_thread(itemid):\n",
    "        # save data locally as pandas data frame\n",
    "        success = save_data(itemid)\n",
    "    \n",
    "        # delete temp data\n",
    "        if success:\n",
    "            for f in os.listdir(DATA_DIR_TMP):\n",
    "                shutil.rmtree(os.path.join(DATA_DIR_TMP, f), ignore_errors=True)\n",
    "    \n",
    "        print(f\"======= Completed download of {itemid} in {round(time.time() - t_init, 2)} seconds ========\")\n",
    "    \n",
    "        return\n",
    "    \n",
    "    thread = threading.Thread(target=save_and_delete_data_thread, args=(itemid,))\n",
    "    thread.start()\n",
    "    \n",
    "def download_data(itemid, n_threads=10):\n",
    "    item = internetarchive.get_item(itemid)\n",
    "        \n",
    "    with Capturing() as all_files:\n",
    "        item.download(dry_run=True)\n",
    "    \n",
    "    if os.path.isdir(os.path.join(DATA_DIR_TMP, itemid)):\n",
    "        downloaded = [f'{URL_BASE}{itemid}/{fn}' for fn in os.listdir(os.path.join(DATA_DIR_TMP, itemid))]\n",
    "        to_download = list(set(all_files).difference(downloaded))\n",
    "    else:\n",
    "        to_download = all_files\n",
    "    \n",
    "    print(f'Downloading {len(to_download)} files')\n",
    "    \n",
    "    to_download = [s.split('/')[-1] for s in to_download]\n",
    "    \n",
    "    splits = np.array_split(to_download, n_threads)\n",
    "                \n",
    "    def download_thread_function(split):\n",
    "        if len(split) == 0:\n",
    "            return\n",
    "        item.download(files=list(split), destdir=DATA_DIR_TMP)\n",
    "\n",
    "    threads = list()\n",
    "    for i in range(n_threads):\n",
    "        x = threading.Thread(target=download_thread_function, args=(splits[i],))\n",
    "        threads.append(x)\n",
    "        x.start()\n",
    "    \n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    \n",
    "def save_data(itemid):\n",
    "    dfs = []\n",
    "    fns = [fn for fn in os.listdir(os.path.join(DATA_DIR_TMP, itemid)) if('.json' in fn)]\n",
    "    \n",
    "    t = time.time() \n",
    "    print(f\"======= Loading {len(fns)} files ========\")\n",
    "    for i, fn in enumerate(fns):\n",
    "        try:\n",
    "            with gzip.open(os.path.join(DATA_DIR_TMP, itemid, fn), \"rb\") as f:\n",
    "                data = [json.loads(line) for line in f]\n",
    "                dfs += [pd.DataFrame(data)[COLS]]\n",
    "        except:\n",
    "            print('Error on ' + fn)\n",
    "            with open(os.path.join(DATA_DIR_TMP, FAILURES_FILE), 'a') as f_failures:\n",
    "                f_failures.write(f'{itemid}/{fn}\\n')\n",
    "        \n",
    "        if (i % 50 == 0 and i > 0):\n",
    "            t_new = time.time()\n",
    "            print(f\"Loaded {i} files in {round(t_new - t, 2)} seconds\")\n",
    "            t = t_new\n",
    "    \n",
    "    t = time.time()\n",
    "    df = pd.concat(dfs)\n",
    "    df.to_csv(get_csv(itemid))\n",
    "    print(f\"======= Saved data in {round(time.time() - t, 2)} seconds =======\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def redownload_failures():\n",
    "    with open('failures.txt', 'r') as f:\n",
    "        files = f.readlines()\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc8aac38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Downloading data for enwiki-20190301-pages-meta-history1.xml-p10p2062_html_dlab =======\n",
      "Already downloaded.\n",
      "======= Downloading data for enwiki-20190301-pages-meta-history1.xml-p11003p12689_html_dlab =======\n",
      "Already downloaded.\n",
      "======= Downloading data for enwiki-20190301-pages-meta-history1.xml-p12690p14358_html_dlab =======\n",
      "Already downloaded.\n",
      "======= Downloading data for enwiki-20190301-pages-meta-history1.xml-p14359p15922_html_dlab =======\n",
      "Already downloaded.\n",
      "======= Downloading data for enwiki-20190301-pages-meta-history1.xml-p15923p17674_html_dlab =======\n",
      "Already downloaded.\n",
      "======= Downloading data for enwiki-20190301-pages-meta-history1.xml-p17675p19356_html_dlab =======\n",
      "Already downloaded.\n",
      "======= Downloading data for enwiki-20190301-pages-meta-history1.xml-p19357p21035_html_dlab =======\n",
      "Already downloaded.\n",
      "======= Downloading data for enwiki-20190301-pages-meta-history1.xml-p2063p4058_html_dlab =======\n",
      "Already downloaded.\n",
      "======= Downloading data for enwiki-20190301-pages-meta-history1.xml-p21036p22590_html_dlab =======\n",
      "Already downloaded.\n",
      "======= Downloading data for enwiki-20190301-pages-meta-history1.xml-p22591p24286_html_dlab =======\n",
      "Downloading 1332 files\n",
      "======= Downloaded Data in 1974.67 seconds ========\n",
      "======= Loading 1480 files ========\n",
      "Loaded 50 files in 71.96 seconds\n",
      "Loaded 100 files in 83.91 seconds\n",
      "Loaded 150 files in 59.35 seconds\n",
      "Loaded 200 files in 65.06 seconds\n",
      "Error on 1202000.json.gz\n",
      "Loaded 250 files in 65.08 seconds\n",
      "Error on 743000.json.gz\n",
      "Loaded 300 files in 63.39 seconds\n",
      "Error on 1271000.json.gz\n",
      "Loaded 350 files in 77.43 seconds\n",
      "Error on 1300000.json.gz\n",
      "Loaded 400 files in 56.57 seconds\n",
      "Loaded 450 files in 82.91 seconds\n",
      "Loaded 500 files in 85.22 seconds\n",
      "Loaded 550 files in 54.79 seconds\n",
      "Loaded 600 files in 66.55 seconds\n",
      "Loaded 650 files in 74.85 seconds\n",
      "Error on 88000.json.gz\n",
      "Loaded 700 files in 71.69 seconds\n",
      "Loaded 750 files in 75.13 seconds\n",
      "Loaded 800 files in 72.91 seconds\n",
      "Error on 1239000.json.gz\n",
      "Loaded 850 files in 74.43 seconds\n",
      "Loaded 900 files in 69.52 seconds\n",
      "Error on 1406000.json.gz\n",
      "Loaded 950 files in 71.81 seconds\n",
      "Loaded 1000 files in 82.81 seconds\n",
      "Loaded 1050 files in 70.5 seconds\n",
      "Loaded 1100 files in 58.44 seconds\n",
      "Loaded 1150 files in 68.45 seconds\n",
      "Error on 207000.json.gz\n",
      "Loaded 1200 files in 68.47 seconds\n",
      "Loaded 1250 files in 73.47 seconds\n",
      "Loaded 1300 files in 69.63 seconds\n",
      "Loaded 1350 files in 69.86 seconds\n",
      "Loaded 1400 files in 80.51 seconds\n",
      "Error on 1138000.json.gz\n",
      "Loaded 1450 files in 61.83 seconds\n",
      "Error on 343000.json.gz\n",
      "======= Saved data in 7.57 seconds =======\n",
      "======= Completed download of enwiki-20190301-pages-meta-history1.xml-p22591p24286_html_dlab in 4066.32 seconds ========\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10):\n",
    "    get_data(itemids[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6e3ced",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:timing] *",
   "language": "python",
   "name": "conda-env-timing-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
